
If you'd like to follow along with this presentation, you can go to this site.  There are some interactive components that may be of interest.  Plus, if you don't pay attention to me, you can't tell me how boring this presentation is.  

I am Dan Hammer, and I am a first-year graduate student in the department of resource economics.  I am also a research fellow at the World Resources Institute, where I work on a system to detect monthly forest clearing activity at 1km spatial resolution that I developed with Robin, who's visiting the West Coast after a month training our partners in Indonesia.  The system, called FORMA, filters massive streams and many layers of satellite imagery toward a tractable estimate of forest clearing activity, something that can actually be used by researchers or even policymakers.  The raw and intermediate data for this effort accounts for about 10TB of disk space, which is roughly equivalent to the entire printed collection of the US Library of Congress.  

In this presentation, I will briefly discuss the detection and classification algorithms, as well as the completely open-source system we built to process the raw data.  I will then talk, even more briefly, about a few applications of the data -- both for economic research and to support local conservation efforts.  But first, I'll show an animation of the output, so that you know what it looks like.

FORMA is currently operational for the full humid tropics at 1km spatial resolution, with planned improvements in the short-term to 500m resolution.  We can zoom into an area in Indonesia, Sumatra, Riau, and an area that has been particularly hard-hit by clearing activity.  This area is approximately 300km on each side, with each pixel representing a 1km square.  The green area indicates forest as of the year 2000; and the brown indicates cumulative forest clearing between 2000 and 2005, as identified by researchers at South Dakota State University.  The SDSU data set is our training data set, and the period 2000-2005 is our training period.  In January 2006, our detection algorithms kick in, providing a probability of forest clearing activity within the pixel between December 2005 and the specified month.  Red values indicate a high probability of deforestation; yellow values indicate a lower probability.  The images look like a spreading infection -- which actually sort of guides how we approach this detection algorithm. The cool part, I think, about spatial data is that you can actually see data, each pixel is colored with the integer value of the output.

This system is meant to complement higher resolution detection algorithms, of which there are many.  These systems offer a very good ex-post assessment of deforestation, but update less frequently -- once every couple of years in cloudy areas.  FORMA, on the other hand, is an alert tool, which classifies the dynamic transition from forest to non-forest, rather than classifying static images and comparing the difference.  That is, we look for breaks or shifts in the spectral characteristics of the pixel through space and time, something to indicate a change in forest cover, based on the way that these patterns are matched to historical deforestation.  Between 2000 and 2005, we characterize the spectral time-series for each pixel and its neighbors, looking for, say, a persistent and extraseasonal downshift in the NDVI -- an index of vegetation density.  We use techniques from macroeconomics to separate the cyclical and trend components to appropriately identify a structural break in the time-series.  We also collect information on the frequency and the intensity of fires through time, trying to sort out whether the fires were generated by independent, random events -- or whether they reflect a generating process that is indicative of intentional clearing activity.  One of my friends is a statistician who wrote a paper on the time between soccer goals, showing that they were not independent events; that teams get some momentum, the game gets more active, immediately following a goal.  We use a similar framework to tag a pixel with a process that corresponds to extra activity.  We pull all of these characteristics together to examine how they're clustered through space and time, and then match them with the best available data on pan-tropical deforestation to tune or train our model to peroperly classify the pixels after the training period.  

The type of deforestation that we are able to detect is limited by the spatial resolution.  At 1km or 500m resolution, we are only able to pick up large-scale forest clearing -- associated with plantation-sized plots.  That said, when we classify a pixel as having experienced clearing activity, we are 95% accurate.  There are very few false positives, which is an important attribute for an alert system.  There are, however, a lot of false negatives -- we miss extractive logging and small scale clearing, so we make no claims on areal estimates.  To give you a sense of the accuracy of the system, we have overlaid FORMA and PRODES, the annual Brazilian forest monitoring system. PRODES is in black outlines, and FORMA is the red-yellow color scheme; and it's clear that FORMA closely tracks PRODES after the training period.  

The only way to analyze this amount of imagery is to split and process chunks in parallel.  However, the workflow is not naturally parallelizable; we need to retain the intrinsic spatiotemporal arrangement of the data to track the dynamic patterns.  That is, we cannot analyze each pixel and each month separately; in some sense, at some stage, we have to compare pixel attributes across space and time.  To do so, we have leveraged a variety of open-source technologies, and the Amazon.com cloud computation platform.  We are searching for the best combination, so we've built two separate systems to do the same thing.  One using primarily Python, relying heavily on the scientific python module.  To distribute processing we rely on Amazon's simple queue service.  The other is based in Clojure, and we rely on Hadoop to distribute processing behind the scense on the Amazon cluster and Mahout to do parallelized machine learning.  The Python workflow is more accessible to the general research community; but the Clojure workflow is amazingly powerful.  Python is a workhorse car, like a VW, which gets the job done; the Clojure system is like a Ferarri, amazingly fast, but it blows to hell if sand gets in the engine.  This is, by the way, the same processing setup for Twitter Analytics.  Both, however, are open-source, very cheap, and totally transparent -- in fact, every part of this workflow from the download of the data to the making of this presentation relies only on open-source projects.  

Now the interesting part: what can we do with these data?  I am a true believer in the power of data transparency; it seems clear to me that it is a necessary component of conservation.  But it's not sufficient.  Big changes are predicated on directed study of the drivers of deforestation and the efficacy of conservation efforts.  We can use these data to examine the short-term dynamics of clearing activity.  We can link the rates of clearing to economic indicators, such as interest rates or agricultural prices.  We can also look at regional, spatial patterns of clearing to detect displacement of clearing as a result of 


* Introduction

My name is Dan Hammer, and I am first-year graduate student in the department of agricultural and resource economics at Cal. 


 the time I am done with this ordeal, I will be schooled up to my ears.  Like my academic background, this project is an exercise in economics, statistics, remote sensing, and spatial analysis.  

* how it is different from other monitoring tools

We analyze the time-series, distilling a 

* why this is important to near real time detection

* why we are able to do this


study of the impact of short-term economic variables on clearing activity requires data with rapid turnaround.  
We are forced to accept a more limited scope in the type of clearing that we study, in order to examine the short-term impact of, say, interest rates on clearing activity.  We are only able to study relatively large-scale clearing - like the sort to make way for palm oil plantations - instead of the clearing that is associated with small-scale timber extraction.  Still, this sort of clearing activity accounts for a large proportion of deforestation in the tropics, and especially in Brazil and Indonesia.  

* Applications

These data can be used in a variety of capacities to increase our understanding of our interaction with our environment.  

** dissemination

** research

Preliminary results. Study the impact of protected areas on clearing rates. 

Interesting studies.  With more available data, we can intensify resarch in production theory.

The generating process of deforestation can be characterized by three dimensions: space, time, and space-time.  A quick aside, here: there is some use in thinking about this problem in terms of vector spaces to provide a context or framework to the mechanics of the subsequent classification methods.  But that is for another day.  In two dimensional space, deforestation can be identified by comparing certain characteristics of the pixel to the value of that characteristic in other, like pixels.  Consider, for example, how _green_ the pixel appears.  If the pixel is less green than similar, nearby pixels, then we may classify it differently.  Then there is the time dimension: a comparison between the way it looks now versus the way it looked three months ago, and the track it took to get between the two values.  Then, finally, we look at the interaction between space and time; the way in which the indicators of some underlying generating process (like a tractor generates cleared land) had traveled through space and time.  Given the relatively arbitrary grid laid across the landscape by MODIS to define the raster, this process can provide insight into the appropriate interpretation of pixel-level characteristics. 


We classify the dynamic transition from forest to non-forest.  This is how our system differs from other idneitification techniques currently available.  Instead of looking at a static cross-seciton, we examine the transition between images -- how the spectral characteristics of the pixels change through space and time. 



All raw data are freely available.  We just layer and analyze the data, using entirely open-source programming languages to identify clearing activity.  


We characterize and then classify the transition from forest to non-forest.  We use the entire, articulated time-series to characterize deforestation as a dynamic process.  Each time-series show seasonality and a normal pattern of fires.  We look for extra-seasonal and semi-persistent breaks in the spectral attributes of the pixel.  The notion of persistence, though, requires a certain attention to variation through time and, for the fire process, through space.  That is, 

Less weight to each observation; taken in context, looking for unexpected breaks or changes in the spectral characteristics. Different for each locale.  


We characterize and classify the dynamic transition from forest to non-forest.  Deforestation is a process that moves through both space and time.  Instead of classifying attributes from each image individually, we examine the pattern as it emerges through both space and time.  As an example, consider the 

The classification algorithms "learn" what the characteristic time-series associated with deforestation actually look like.  We can classify this particular pattern by sight: it has some seasonality, a big downward break, and then a distinct lack of seasonality.  

Seasonal 

cool part about this study, spatial data in general, is taht you can actually see the data -- and it makes sense, like the forma animation maps.  Its just the raw output, with a bit of color indexing.

